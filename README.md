# Credit Card Fraud Detection

!()

Introduction Goes Here

### The Credit Card Fraud Dataset

The [Credit Card Fraud Detection dataset](https://www.kaggle.com/mlg-ulb/creditcardfraud) from Kaggle contains 284,807 credit card transactions made in September 2013 by credit cardholders.  Of the 284,807 transactions, 492 (0.172%) are fraudulent.  

!(/Images/Target_Imbalance.PNG)

The dataset consists of 31 columns of numerical data.  Three of the columns are time (number of seconds between the first transaction in the dataset and the current transaction), amount of transaction, and class (1 = fraudulent; 0 = not fraudulent).  The remaining columns are designated as V1, V2, etc.  The variables in these columns were obtained using Principal Component Analysis (PCA) to transform data about each transaction.  We do not know what the original data features are because that information is confidential but we can see that several of the top 10 most predictive features are correlated with each other.

!(/Images/Correlation_Matrix.PNG)

### Creating the Best Machine Learning Model

We set out to discover what machine learning model might be the best for predicting credit card fraud in the real world.  We used several tools in the data science toolbox to find our best model.

#### Amazon SageMaker Autopilot

Our first step was to use Amazon SageMaker Autopilot to select and build several machine learning (ML) models for the credit card fraud dataset.  Autopilot, as the name suggests, automates the data preprocessing and model selectiong, building, and tuning.  We modeled our code on that in the GitHub repository [Direct Marketing with Amazon SageMaker AutoPilot](https://github.com/aws/amazon-sagemaker-examples/blob/master/autopilot/sagemaker_autopilot_direct_marketing.ipynb).  

We ran two instances of Autopilot.  For the first, we accepted the default inputs and let Autopilot select the best type of model and metric for our data.  Autopilot correctly selected binary classification for the model type and selected F1 score for the metric used to measure which model would be best.  

For imbalanced datasets like the credit card fraude dataset, the area under the Receiver Operating Characteristic (ROC) curve (AUC) is a better metric use. The AUC measures the true positive rate versus the false posictive rate to judege model performance.  For our second Autopilot instance, we used AUC as the defining metric.  

In both notebooks, the code creates an Amazon S3 bucket for the ZIP file containing the credit card fraud data CSV file.  Then it splits the data into training and testing sets and uploads them to the Amazon S3 bucket.  Next the code configures and launches an Autopilot job to process the data, create a valdiation set, determine which five ML models would work best for the dataset.  Autopilot tunes the top models to determine the optimal hyperparameters.  

After the Autopilot job completes, the code deploys an inference pipeline to create a model from the top candidate returned by Autopilot.  It then uses batch transform to remove noise and other interferance that might affect model performance and generate predictions using the ML model.  Other candidate models can be viewed, and the location of two autogenerated notebooks, a Data Exploration Notebook and a Candidate Definition Notebook are given.  The final line of code cleans up the files created by Autopilot during processing.

The Data Exploration Notebook gives an overview of the data.  We pulled our data exploration charts from this notebook.  The Candidate Defintion Notebook shows the code used by Autopilot for the top performing model, giving us an opportunity to further refine the model.

#### Home Grown Models

To see if Autopilot's model really is the best, we built five of our own models:

1. Decision Tree
2. Gradient Boosting
3. Random Forest
4. Bagging Classifier
4. XGBoost

### Model Evaluation

For the model selected by Autopilot based on F1 score, the F1 score is 0.6461499929428101, accuracy is 0.9984899759292603, and the AUC is 0.9936299920082092.  For the model selected based on AUC, F1 score is 0.4532400071620941, accuracy is 0.9966800212860107, and the AUC is 0.9926300048828125.

### Resources

Amazon Web Serices. *AWS Deploy and Inference Pipeline*. https://docs.aws.amazon.com/sagemaker/latest/dg/inference-pipelines.html. Retrieved November 21, 2021.

Amazon Web Serices. *AWS Use Batch Transform*. https://docs.aws.amazon.com/sagemaker/latest/dg/batch-transform.html. Retrieved November 21, 2021.

Karpur, Ajay, et al. *Customer Churn Prediction with Amazon SageMaker Autopilot*. https://github.com/aws/amazon-sagemaker-examples/blob/master/autopilot/autopilot_customer_churn.ipynb. Retrieved November 22, 2021.

Samuel, James. *Training Models to Detect Credit Card Frauds with Amazon SageMaker Autopilot*. https://towardsdatascience.com/training-models-to-detect-credit-card-frauds-with-amazon-sagemaker-autopilot-d49a6b667b2e. Retrieved November 21, 2021.

Severtson, Roald Bradley, et al. *Direct Marketing with Amazon SageMaker AutoPilot*. https://github.com/aws/amazon-sagemaker-examples/blob/master/autopilot/sagemaker_autopilot_direct_marketing.ipynb. Retrieved November 21, 2021.
